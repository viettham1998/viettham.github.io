<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Viet Tham Huynh </title> <meta name="author" content="Viet Tham Huynh"> <meta name="description" content="Viet-Tham Huynh Homepage. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/fav.ico?89ee8eb0f36c50bef024e1ca8be2ff7c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://viettham1998.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Viet Tham</span> Huynh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">My CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huynh2024lumos" class="col-sm-8"> <div class="title">LUMOS-DM: Landscape-Based Multimodal Scene Retrieval Enhanced by Diffusion Model</div> <div class="author"> Viet-Tham Huynh , Trong-Thuan Nguyen , Quang-Thuc Nguyen , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Mai-Khiem Tran, Tam V Nguyen, Minh-Triet Tran' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In International Conference on Multimedia Modeling</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-53302-0_11" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Information retrieval is vital in our daily lives, with applications ranging from job searches to academic research. In today’s data-driven world, efficient and accurate retrieval systems are crucial. Our research focuses on video data, using a system called LUMOS-DM: Landscape-based Multimodal Scene Retrieval Enhanced by Diffusion Model. This system leverages Vision Transformer and Diffusion Models, taking user-generated sketch images and text queries as input to generate images for video retrieval. Initial testing on a dataset of 100 h of global landscape videos achieved an 18.78% at Top-20 accuracy rate and 36.45% at Top-100 accuracy rate. Additionally, video retrieval has various applications, including generating data for advertising and marketing. We use a multi-modal approach, combining sketch and text descriptions to enhance video content retrieval, catering to a wide range of user needs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2024lumos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LUMOS-DM: Landscape-Based Multimodal Scene Retrieval Enhanced by Diffusion Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Viet-Tham and Nguyen, Trong-Thuan and Nguyen, Quang-Thuc and Tran, Mai-Khiem and Nguyen, Tam V and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Multimedia Modeling}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{145--158}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huynh2023mobilenet" class="col-sm-8"> <div class="title">MobileNet-SA: Lightweight CNN with Self Attention for Sketch Classification</div> <div class="author"> Viet-Tham Huynh , Trong-Thuan Nguyen , Tam V Nguyen , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Minh-Triet Tran' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Pacific-Rim Symposium on Image and Video Technology</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-981-97-0376-0_9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Sketch classification plays a crucial role across diverse domains, including image retrieval, artistic style analysis, and content-based image retrieval. While CNNs have demonstrated remarkable success in various image-related tasks, the computational complexity of large models poses challenges in resource-constrained environments. To address this concern, we propose MobileNet-SA, a novel lightweight model that seamlessly integrates a self-attention module into the MobileNet architecture, with a specific focus on enhancing sketch classification performance. The MobileNet-SA model leverages the inherent efficiency of lightweight CNN while harnessing the power of self-attention mechanisms to effectively capture spatial dependencies and enrich feature representations within sketch data. In our experiments, MobileNet-SA achieves state-of-the-art results, demonstrating an impressive accuracy of 93.5% on the challenging SketchyCOCO dataset and 96.7% on the GM-Sketch dataset. We thoroughly evaluate the model’s performance across diverse sketch classes, confirming its robustness and generalization capabilities, which make it well-suited for real-world applications where input sketches may exhibit significant variations. Our research indicates that MobileNet-SA not only outperforms existing methods but also offers an efficient and interpretable solution for sketch classification tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2023mobilenet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MobileNet-SA: Lightweight CNN with Self Attention for Sketch Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Viet-Tham and Nguyen, Trong-Thuan and Nguyen, Tam V and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Pacific-Rim Symposium on Image and Video Technology}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{110--123}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huynh2023sketch2reality" class="col-sm-8"> <div class="title">Sketch2Reality: Immersive 3D Indoor Scene Synthesis via Sketches</div> <div class="author"> Viet-Tham Huynh , Tam V Nguyen , and Minh-Triet Tran </div> <div class="periodical"> <em>In Proceedings of the 12th International Symposium on Information and Communication Technology</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3628797.3628991" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Sketching indoor scenes is helpful in daily activities as it allows for quick visualization and planning of room layouts, furniture arrangements, design ideas, or scene creation for games and entertainment. This motivates our proposal of Sketch2Reality, a system to simplify the creation of immersive 3D indoor scenes from 2D sketch images. Users sketch their desired scene, and our system identifies sketched objects and their positions, then retrieves and populates corresponding 3D models into the generating 3D scene. Users can then modify the scene, rearrange furniture, adjust lighting, and add or remove objects. Integration with Virtual Reality technology allows users to experience and interact with the scene realistically. Our experiments with three groups of users with different experience levels in 3D scene design and creation demonstrate the efficiency and usefulness of our solution. Sketch2Reality empowers users to dynamically bring their ideas to life, combining sketching, AI assistance for 3D generation, and VR for enhanced creativity and design exploration.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2023sketch2reality</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sketch2Reality: Immersive 3D Indoor Scene Synthesis via Sketches}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Viet-Tham and Nguyen, Tam V and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 12th International Symposium on Information and Communication Technology}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{863--869}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huynh2023light" class="col-sm-8"> <div class="title">Light-weight Sketch Recognition with Knowledge Distillation</div> <div class="author"> Viet-Tham Huynh , Tam V Nguyen , and Minh-Triet Tran </div> <div class="periodical"> <em>In 2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10289002" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recognizing hand-drawn sketches is a promising starting point for various applications, such as assisting artists in creating 3D environments for games or virtual environment scenes quickly and efficiently from concept arts. In addition, by understanding drawings, we can generate 3D models that can be used for further design and development. Thus, in this paper, we aim to develop a novel lightweight network that can accurately recognize sketch drawings. We propose a lightweight-yet-efficient neural network based on MobileNetV2 for sketch recognition and employ knowledge distillation to train the proposed model from EfficientNet-B4. To evaluate the accuracy of the proposed method, we collect a dataset of sketches comprising 1800 drawings in 12 categories, ranging from furniture to animals. The experimental results show that our network model achieves an accuracy of 96.7%, with 96.9% precision, 96.7% recall, and 96.7% F1-score. These results demonstrate that the proposed approach has great potential for practical sketch recognition applications, such as interior design or VR scene generation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2023light</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Light-weight Sketch Recognition with Knowledge Distillation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Viet-Tham and Nguyen, Tam V and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tran2023leveraging" class="col-sm-8"> <div class="title">Leveraging Deep Learning and Knowledge Distillation for Enhanced Traffic Anomaly Detection in Transportation Systems</div> <div class="author"> Mai-Khiem Tran , Viet-Tham Huynh , and Minh-Triet Tran </div> <div class="periodical"> <em>In 2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10288989" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>This paper introduces an innovative approach to enhance traffic anomaly detection in transportation systems using deep learning and knowledge distillation. We create a robust dataset from 427 videos containing 1,415 accident-related events, spanning various anomalies like accidents, car crashes, and pedestrian violations. To address real-time anomaly detection challenges, we propose a novel lightweight neural network architecture inspired by EfficientNet-B0, designed for efficient video anomaly detection. Through knowledge distillation, a student model learns from a teacher model’s predictions, resulting in heightened anomaly detection accuracy. Experimental results highlight the approach’s efficacy, with the knowledge-distilled student model consistently outperforming the standalone lightweight network, achieving an accuracy of 94.83% compared to 94.16%. This research offers a practical solution for real-time traffic anomaly detection, which is especially valuable in resource-constrained environments. Fusing a unique dataset, EfficientNet-B0-like structure, lightweight architecture, and knowledge distillation holds significant potential for fostering safer and more efficient transportation systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tran2023leveraging</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Leveraging Deep Learning and Knowledge Distillation for Enhanced Traffic Anomaly Detection in Transportation Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tran, Mai-Khiem and Huynh, Viet-Tham and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le2023textanimar" class="col-sm-8"> <div class="title">TextANIMAR: text-based 3D animal fine-grained retrieval</div> <div class="author"> Trung-Nghia Le , Tam V Nguyen , Minh-Quan Le , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Trong-Thuan Nguyen, Viet-Tham Huynh, Trong-Le Do, Khanh-Duy Le, Mai-Khiem Tran, Nhat Hoang-Xuan, Thang-Long Nguyen-Ho, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>Computers &amp; Graphics</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0097849323001553" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>3D object retrieval is an important yet challenging task that has drawn more and more attention in recent years. While existing approaches have made strides in addressing this issue, they are often limited to restricted settings such as image and sketch queries, which are often unfriendly interactions for common users. In order to overcome these limitations, this paper presents a novel SHREC challenge track focusing on text-based fine-grained retrieval of 3D animal models. Unlike previous SHREC challenge tracks, the proposed task is considerably more challenging, requiring participants to develop innovative approaches to tackle the problem of text-based retrieval. Despite the increased difficulty, we believe this task can potentially drive useful applications in practice and facilitate more intuitive interactions with 3D objects. Five groups participated in our competition, submitting a total of 114 runs. While the results obtained in our competition are satisfactory, we note that the challenges presented by this task are far from fully solved. As such, we provide insights into potential areas for future research and improvements. We believe we can help push the boundaries of 3D object retrieval and facilitate more user-friendly interactions via vision-language technologies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">le2023textanimar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TextANIMAR: text-based 3D animal fine-grained retrieval}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Le, Trung-Nghia and Nguyen, Tam V and Le, Minh-Quan and Nguyen, Trong-Thuan and Huynh, Viet-Tham and Do, Trong-Le and Le, Khanh-Duy and Tran, Mai-Khiem and Hoang-Xuan, Nhat and Nguyen-Ho, Thang-Long and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers \&amp; Graphics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{116}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{162--172}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="do2023news" class="col-sm-8"> <div class="title">News event retrieval from large video collection in Ho Chi Minh City AI challenge 2023</div> <div class="author"> Trong-Le Do , Hai-Dang Nguyen , Quang-Thuc Nguyen , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Mai-Khiem Tran, Viet-Tham Huynh, Cathal Gurrin, Tu V Ninh, Tu-Khiem Le, Thanh Duc Ngo, Tu-Trinh Ngo, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 12th International Symposium on Information and Communication Technology</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3628797.3628940" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Event retrieval from large collections of TV news videos is crucial for efficient information access, enabling researchers, journalists, and the general public to quickly locate and analyze relevant content amidst the vast sea of news coverage, facilitating informed decision-making and a comprehensive understanding of significant events. This paper presents an overview of the AI-driven video retrieval task in Ho Chi Minh City AI Challenge 2023. The competition draws inspiration from internationally recognized competitions, namely the Video Browser Showdown (VBS) and the Lifelog Search Challenge (LSC). Participants are tasked with developing AI models to retrieve specific video segments from a diverse dataset from reputable news channels. The dataset comprises a vast collection of videos, keyframes, object detections, CLIP features, and metadata. It is divided into three packs with a total of 1,270 videos, spanning approximately 360 hours of content. The challenge comprises two groups. Group A is open to students, researchers, and practitioners in artificial intelligence and information retrieval, emphasizing substantial knowledge and experience. Group B is tailored for high school students, focusing on nurturing interest, learning, and engagement among the next generation of AI enthusiasts. The wide variation in the content of queries challenged participants to demonstrate their adaptability and creativity in effectively retrieving diverse events from the extensive TV news video dataset. The winning teams showcased promising solutions by effectively harnessing artificial intelligence and information retrieval techniques to excel in event retrieval from a vast collection of TV news videos.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">do2023news</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{News event retrieval from large video collection in Ho Chi Minh City AI challenge 2023}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Do, Trong-Le and Nguyen, Hai-Dang and Nguyen, Quang-Thuc and Tran, Mai-Khiem and Huynh, Viet-Tham and Gurrin, Cathal and Ninh, Tu V and Le, Tu-Khiem and Ngo, Thanh Duc and Ngo, Tu-Trinh and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 12th International Symposium on Information and Communication Technology}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1011--1017}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le2023sketchanimar" class="col-sm-8"> <div class="title">SketchANIMAR: sketch-based 3D animal fine-grained retrieval</div> <div class="author"> Trung-Nghia Le , Tam V Nguyen , Minh-Quan Le , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Trong-Thuan Nguyen, Viet-Tham Huynh, Trong-Le Do, Khanh-Duy Le, Mai-Khiem Tran, Nhat Hoang-Xuan, Thang-Long Nguyen-Ho, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>Computers &amp; Graphics</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0097849323001644" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The retrieval of 3D objects has gained significant importance in recent years due to its broad range of applications in computer vision, computer graphics, virtual reality, and augmented reality. However, the retrieval of 3D objects presents significant challenges due to the intricate nature of 3D models, which can vary in shape, size, and texture, and have numerous polygons and vertices. To this end, we introduce a novel SHREC challenge track that focuses on retrieving relevant 3D animal models from a dataset using sketch queries and expedites accessing 3D models through available sketches. Furthermore, a new dataset named ANIMAR was constructed in this study, comprising a collection of 711 unique 3D animal models and 140 corresponding sketch queries. Our contest requires participants to retrieve 3D models based on complex and detailed sketches. We receive satisfactory results from eight teams and 204 runs. Although further improvement is necessary, the proposed task has the potential to incentivize additional research in the domain of 3D object retrieval, potentially yielding benefits for a wide range of applications. We also provide insights into potential areas of future research, such as improving techniques for feature extraction and matching and creating more diverse datasets to evaluate retrieval performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">le2023sketchanimar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SketchANIMAR: sketch-based 3D animal fine-grained retrieval}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Le, Trung-Nghia and Nguyen, Tam V and Le, Minh-Quan and Nguyen, Trong-Thuan and Huynh, Viet-Tham and Do, Trong-Le and Le, Khanh-Duy and Tran, Mai-Khiem and Hoang-Xuan, Nhat and Nguyen-Ho, Thang-Long and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers \&amp; Graphics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{116}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{150--161}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le2022chemisim" class="col-sm-8"> <div class="title">Chemisim: A Web-based VR Simulator for Chemistry Experiments</div> <div class="author"> Hoang-Minh Le , Gia-Huy Nguyen , Viet-Tham Huynh , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Minh-Kha Le, Minh-Triet Tran, Tam V Nguyen, Thanh Ngoc-Dat Tran' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9974257" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In developing countries, high schoolers rarely have opportunities to conduct chemical experiments due to the lack of facilities. There-fore, chemistry experiment simulation is an alternative environment for students to do the chemistry lab assignments. Despite the need of creating virtual simulations to expand the application usability, it is challenging to synthesize a realistic environment given the limited computing resources. In this paper, we propose Chemisim, a highly realistic web-based VR laboratory simulation for students with high quality and usability. In particular, we make use of the fluid simulation system to mimic real chemical reactions. The imple-mented simulation was based on the chemistry assignments in the national education system, consulted by chemical teachers. Then we deployed the simulator on the web to promote a wide range of students usage. The system was evaluated by collecting and analyzing feedback from chemical teachers based on four criteria, namely, convenience, realism, functionality, and preferences. Our experimental findings address educational challenges and produce innovative technical solutions to solve them in developing countries.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">le2022chemisim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Chemisim: A Web-based VR Simulator for Chemistry Experiments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Le, Hoang-Minh and Nguyen, Gia-Huy and Huynh, Viet-Tham and Le, Minh-Kha and Tran, Minh-Triet and Nguyen, Tam V and Tran, Thanh Ngoc-Dat}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{850--854}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nguyen2022data" class="col-sm-8"> <div class="title">Data-Driven City Traffic Planning Simulation</div> <div class="author"> Tam V Nguyen , Thanh Ngoc-Dat Tran , Viet-Tham Huynh , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Bao Truong, Minh-Quan Le, Mohit Kumavat, Vatsa S Patel, Mai-Khiem Tran, Minh-Triet Tran' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9974523" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Big cities are well-known for their traffic congestion and high density of vehicles such as cars, buses, trucks, and even a swarm of motorbikes that overwhelm city streets. Large-scale development projects have exacerbated urban conditions, making traffic congestion more severe. In this paper, we proposed a data-driven city traffic planning simulator. In particular, we make use of the city camera system for traffic analysis. It seeks to recognize the traffic vehicles and traffic flows, with reduced intervention from monitoring staff. Then, we develop a city traffic planning simulator upon the analyzed traffic data. The simulator is used to support metropolitan transportation planning. Our experimental findings address traffic planning challenges and the innovative technical solutions needed to solve them in big cities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nguyen2022data</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data-Driven City Traffic Planning Simulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Tam V and Tran, Thanh Ngoc-Dat and Huynh, Viet-Tham and Truong, Bao and Le, Minh-Quan and Kumavat, Mohit and Patel, Vatsa S and Tran, Mai-Khiem and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{859--864}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/img_purple-480.webp 480w,/assets/img/publication_preview/img_purple-800.webp 800w,/assets/img/publication_preview/img_purple-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/img_purple.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="img_purple.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huynh2022tail" class="col-sm-8"> <div class="title">Tail-Aware Sperm Analysis for Transparent Tracking of Spermatozoa</div> <div class="author"> Tuan-Luc Huynh , Huu-Hung Nguyen , Xuan-Nhat Hoang , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Thao Thi Phuong Dao, Tien-Phat Nguyen, Viet-Tham Huynh, Hai-Dang Nguyen, Trung-Nghia Le, Minh-Triet Tran' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In MediaEval 2022 Workshop</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ceur-ws.org/Vol-3583/paper33.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Semen analysis is crucial to determine men’s fertility; however, microscope-based manual spermatozoa evaluation is time-consuming and costly. Therefore, it has become essential to develop computeraided-semen-analysis systems. To facilitate automated spermatozoa analysis, we propose a simple yet efficient framework for tracking sperms and predicting their motility. Different from existing methods, our proposed framework centralizes a new paradigm, dubbed sperm having a tail. We develop a novel tail-aware sperm detection model to advance the detection ability of dense, tiny, and transparent sperm cells. Furthermore, to enhance sperm tracking, a scene change detection technique is utilized to suppress identity assignment errors of similar sperms, resulting in improved sperm motility measurement. Experimental results show that our framework works well with an insignificant trade-off in execution time, which is suitable for the real-time clinical setting requirement.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2022tail</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tail-Aware Sperm Analysis for Transparent Tracking of Spermatozoa}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Tuan-Luc and Nguyen, Huu-Hung and Hoang, Xuan-Nhat and Dao, Thao Thi Phuong and Nguyen, Tien-Phat and Huynh, Viet-Tham and Nguyen, Hai-Dang and Le, Trung-Nghia and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{MediaEval 2022 Workshop}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Viet Tham Huynh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Last updated: July 14, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>