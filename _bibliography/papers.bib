---
---

@string{aps = {American Physical Society,}}

@inproceedings{le2022chemisim,
  bibtex_show={true},
  title={Chemisim: A Web-based VR Simulator for Chemistry Experiments},
  author={Le, Hoang-Minh and Nguyen, Gia-Huy and Huynh, Viet-Tham and Le, Minh-Kha and Tran, Minh-Triet and Nguyen, Tam V and Tran, Thanh Ngoc-Dat},
  abstract={In developing countries, high schoolers rarely have opportunities to conduct chemical experiments due to the lack of facilities. There-fore, chemistry experiment simulation is an alternative environment for students to do the chemistry lab assignments. Despite the need of creating virtual simulations to expand the application usability, it is challenging to synthesize a realistic environment given the limited computing resources. In this paper, we propose Chemisim, a highly realistic web-based VR laboratory simulation for students with high quality and usability. In particular, we make use of the fluid simulation system to mimic real chemical reactions. The imple-mented simulation was based on the chemistry assignments in the national education system, consulted by chemical teachers. Then we deployed the simulator on the web to promote a wide range of students usage. The system was evaluated by collecting and analyzing feedback from chemical teachers based on four criteria, namely, convenience, realism, functionality, and preferences. Our experimental findings address educational challenges and produce innovative technical solutions to solve them in developing countries.},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
  pages={850--854},
  year={2022},
  organization={IEEE},
  preview={img_purple.jpeg},
  html={https://ieeexplore.ieee.org/document/9974257}
}

@inproceedings{huynh2023mobilenet,
  bibtex_show={true},
  selected={true},
  title={MobileNet-SA: Lightweight CNN with Self Attention for Sketch Classification},
  author={Huynh, Viet-Tham and Nguyen, Trong-Thuan and Nguyen, Tam V and Tran, Minh-Triet},
  abstract={Sketch classification plays a crucial role across diverse domains, including image retrieval, artistic style analysis, and content-based image retrieval. While CNNs have demonstrated remarkable success in various image-related tasks, the computational complexity of large models poses challenges in resource-constrained environments. To address this concern, we propose MobileNet-SA, a novel lightweight model that seamlessly integrates a self-attention module into the MobileNet architecture, with a specific focus on enhancing sketch classification performance. The MobileNet-SA model leverages the inherent efficiency of lightweight CNN while harnessing the power of self-attention mechanisms to effectively capture spatial dependencies and enrich feature representations within sketch data. In our experiments, MobileNet-SA achieves state-of-the-art results, demonstrating an impressive accuracy of 93.5% on the challenging SketchyCOCO dataset and 96.7% on the GM-Sketch dataset. We thoroughly evaluate the modelâ€™s performance across diverse sketch classes, confirming its robustness and generalization capabilities, which make it well-suited for real-world applications where input sketches may exhibit significant variations. Our research indicates that MobileNet-SA not only outperforms existing methods but also offers an efficient and interpretable solution for sketch classification tasks.},
  booktitle={Pacific-Rim Symposium on Image and Video Technology},
  pages={110--123},
  year={2023},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-981-97-0376-0_9},
  preview={img_purple.jpeg}
}

@inproceedings{huynh2023sketch2reality,
  bibtex_show={true},
  title={Sketch2Reality: Immersive 3D Indoor Scene Synthesis via Sketches},
  author={Huynh, Viet-Tham and Nguyen, Tam V and Tran, Minh-Triet},
  booktitle={Proceedings of the 12th International Symposium on Information and Communication Technology},
  pages={863--869},
  year={2023},
  preview={img_purple.jpeg}
}

@inproceedings{huynh2024lumos,
  bibtex_show={true}, 
  selected={true},
  title={LUMOS-DM: Landscape-Based Multimodal Scene Retrieval Enhanced by Diffusion Model},
  author={Huynh, Viet-Tham and Nguyen, Trong-Thuan and Nguyen, Quang-Thuc and Tran, Mai-Khiem and Nguyen, Tam V and Tran, Minh-Triet},
  booktitle={International Conference on Multimedia Modeling},
  pages={145--158},
  year={2024},
  organization={Springer},
  preview={img_purple.jpeg}
}

@inproceedings{nguyen2022data,
  bibtex_show={true},
  selected={true},
  title={Data-Driven City Traffic Planning Simulation},
  author={Nguyen, Tam V and Tran, Thanh Ngoc-Dat and Huynh, Viet-Tham and Truong, Bao and Le, Minh-Quan and Kumavat, Mohit and Patel, Vatsa S and Tran, Mai-Khiem and Tran, Minh-Triet},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
  pages={859--864},
  year={2022},
  organization={IEEE},
  preview={img_purple.jpeg}
}

@inproceedings{huynh2023light,
  bibtex_show={true},
  selected={true},
  title={Light-weight Sketch Recognition with Knowledge Distillation},
  author={Huynh, Viet-Tham and Nguyen, Tam V and Tran, Minh-Triet},
  booktitle={2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)},
  pages={1--6},
  year={2023},
  organization={IEEE},
  preview={img_purple.jpeg}
}

@inproceedings{tran2023leveraging,
  bibtex_show={true},
  title={Leveraging Deep Learning and Knowledge Distillation for Enhanced Traffic Anomaly Detection in Transportation Systems},
  author={Tran, Mai-Khiem and Huynh, Viet-Tham and Tran, Minh-Triet},
  booktitle={2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)},
  pages={1--6},
  year={2023},
  organization={IEEE},
  preview={img_purple.jpeg}
}

@inproceedings{huynh2022tail,
  bibtex_show={true},
  title={Tail-Aware Sperm Analysis for Transparent Tracking of Spermatozoa},
  author={Huynh, Tuan-Luc and Nguyen, Huu-Hung and Hoang, Xuan-Nhat and Dao, Thao Thi Phuong and Nguyen, Tien-Phat and Huynh, Viet-Tham and Nguyen, Hai-Dang and Le, Trung-Nghia and Tran, Minh-Triet},
  booktitle={MediaEval 2022 Workshop},
  year={2022},
  preview={img_purple.jpeg}
}

@article{le2023textanimar,
  bibtex_show={true},
  title={TextANIMAR: text-based 3D animal fine-grained retrieval},
  author={Le, Trung-Nghia and Nguyen, Tam V and Le, Minh-Quan and Nguyen, Trong-Thuan and Huynh, Viet-Tham and Do, Trong-Le and Le, Khanh-Duy and Tran, Mai-Khiem and Hoang-Xuan, Nhat and Nguyen-Ho, Thang-Long and others},
  journal={Computers \& Graphics},
  volume={116},
  pages={162--172},
  year={2023},
  publisher={Elsevier},
  preview={img_purple.jpeg}
}

@inproceedings{do2023news,
  bibtex_show={true},
  title={News event retrieval from large video collection in Ho Chi Minh City AI challenge 2023},
  author={Do, Trong-Le and Nguyen, Hai-Dang and Nguyen, Quang-Thuc and Tran, Mai-Khiem and Huynh, Viet-Tham and Gurrin, Cathal and Ninh, Tu V and Le, Tu-Khiem and Ngo, Thanh Duc and Ngo, Tu-Trinh and others},
  booktitle={Proceedings of the 12th International Symposium on Information and Communication Technology},
  pages={1011--1017},
  year={2023},
  preview={img_purple.jpeg}
}

@article{le2023sketchanimar,
  bibtex_show={true},
  title={SketchANIMAR: sketch-based 3D animal fine-grained retrieval},
  author={Le, Trung-Nghia and Nguyen, Tam V and Le, Minh-Quan and Nguyen, Trong-Thuan and Huynh, Viet-Tham and Do, Trong-Le and Le, Khanh-Duy and Tran, Mai-Khiem and Hoang-Xuan, Nhat and Nguyen-Ho, Thang-Long and others},
  journal={Computers \& Graphics},
  volume={116},
  pages={150--161},
  year={2023},
  publisher={Elsevier},
  preview={img_purple.jpeg}
}

